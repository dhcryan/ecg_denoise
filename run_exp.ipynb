{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wfdb\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from scipy.signal import resample_poly\n",
    "import wfdb\n",
    "import math\n",
    "import _pickle as pickle\n",
    "\n",
    "def Data_Preparation(samples):\n",
    "    print('Getting the Data ready ...')\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 1234\n",
    "    np.random.seed(seed=seed)\n",
    "\n",
    "    # Load QT Database\n",
    "    with open('data/QTDatabase.pkl', 'rb') as input:\n",
    "        qtdb = pickle.load(input)\n",
    "\n",
    "    print(f\"[INFO] Loaded QTDatabase with {len(qtdb.keys())} signals\")\n",
    "\n",
    "    # Load combined noise\n",
    "    with open('data/CombinedNoise.pkl', 'rb') as input:\n",
    "        combined_noise = pickle.load(input)\n",
    "    print(f\"[INFO] Loaded CombinedNoise with {len(combined_noise)} channels\")\n",
    "\n",
    "    #####################################\n",
    "    # Data split\n",
    "    #####################################\n",
    "    test_set = ['sel123', 'sel233', 'sel302', 'sel307', 'sel820', 'sel853', \n",
    "                'sel16420', 'sel16795', 'sele0106', 'sele0121', 'sel32', 'sel49', \n",
    "                'sel14046', 'sel15814']\n",
    "\n",
    "    beats_train = []\n",
    "    beats_test = []\n",
    "    valid_train_indices = []  # To keep track of valid indices in training data\n",
    "    valid_test_indices = []   # To keep track of valid indices in test data\n",
    "    sn_train = []\n",
    "    sn_test = []\n",
    "    noise_indices_train = []\n",
    "    noise_indices_test = []    \n",
    "    \n",
    "    skip_beats = 0\n",
    "    # samples = 512\n",
    "    qtdb_keys = list(qtdb.keys())\n",
    "\n",
    "    print(f\"[INFO] Processing QTDatabase, {len(qtdb_keys)} signals to process.\")\n",
    "# b_np.shape는 (512,)로, 패딩을 포함한 전체 샘플 크기가 512임을 알 수 있습니다.\n",
    "    for signal_name in qtdb_keys:\n",
    "        for b_idx, b in enumerate(qtdb[signal_name]):\n",
    "            b_np = np.zeros(samples)\n",
    "            b_sq = np.array(b)\n",
    "\n",
    "            init_padding = 16\n",
    "            if b_sq.shape[0] > (samples - init_padding):\n",
    "                skip_beats += 1\n",
    "                continue\n",
    "# 이 평균값을 b_sq의 각 값에서 빼는 과정은 신호의 중앙화 작업입니다. 즉, 신호의 값들이 배열의 양 끝 값의 평균을 기준으로 대칭적으로 배치되도록 변환됩니다.\n",
    "# 이 계산을 통해 신호의 첫 값과 마지막 값에 대한 편향을 제거하고, 신호를 중앙으로 이동시키는 효과가 있습니다.\n",
    "            b_np[init_padding:b_sq.shape[0] + init_padding] = b_sq - (b_sq[0] + b_sq[-1]) / 2\n",
    "\n",
    "            if signal_name in test_set:\n",
    "                beats_test.append(b_np)\n",
    "                valid_test_indices.append(len(beats_test) - 1)  # Track valid test beat index\n",
    "            else:\n",
    "                beats_train.append(b_np)\n",
    "                valid_train_indices.append(len(beats_train) - 1)  # Track valid train beat index\n",
    "\n",
    "        print(f\"[DEBUG] Processed signal {signal_name}, total beats in train: {len(beats_train)}, total beats in test: {len(beats_test)}\")\n",
    "\n",
    "    #####################################\n",
    "    # Adding noise to train and test sets\n",
    "    #####################################\n",
    "    print(f\"[INFO] Adding noise to train and test sets\")\n",
    "    # Random scaling factor for train and test\n",
    "    # size=len(beats_train): beats_train의 길이만큼 난수를 생성합니다. 즉, beats_train에 있는 심박 데이터의 개수와 동일한 수의 난수를 생성합니다.\n",
    "    rnd_train = np.random.randint(low=20, high=200, size=len(beats_train)) / 100\n",
    "    noise_index = 0\n",
    "    # Adding noise to train\n",
    "    # https://chatgpt.com/g/g-cKXjWStaE-python/c/66e1471b-57b4-8006-b921-233e7803fcab\n",
    "    for beat_idx, beat in enumerate(beats_train):\n",
    "        if (beat_idx // 10) % 2 == 0:\n",
    "            selected_channel = beat_idx % 2  # 0과 1을 번갈아 선택\n",
    "        else:\n",
    "            selected_channel = (beat_idx + 1) % 2  # 반대 순서로 선택\n",
    "\n",
    "        # 노이즈 조합도 순차적으로 선택, 주기적으로 변화를 줌 (매 8회 주기)\n",
    "        noise_combination_idx = (beat_idx % 7) + 1  # 1부터 7까지 순차적으로 선택\n",
    "        noise = combined_noise[selected_channel][:, noise_combination_idx]\n",
    "        noise_segment = noise[noise_index:noise_index + samples]\n",
    "        beat_max_value = np.max(beat) - np.min(beat)\n",
    "        noise_max_value = np.max(noise_segment) - np.min(noise_segment)\n",
    "        Ase = noise_max_value / beat_max_value\n",
    "        alpha = rnd_train[beat_idx] / Ase\n",
    "        signal_noise = beat + alpha * noise_segment\n",
    "        sn_train.append(signal_noise)\n",
    "        noise_indices_train.append(noise_combination_idx)  # 노이즈 인덱스 저장\n",
    "        noise_index += samples\n",
    "        if noise_index > (len(noise) - samples):\n",
    "            noise_index = 0\n",
    "\n",
    "                \n",
    "    # Adding noise to test\n",
    "    noise_index = 0\n",
    "    rnd_test = np.random.randint(low=20, high=200, size=len(beats_test)) / 100\n",
    "    # Saving the random array so we can use it on the amplitude segmentation tables\n",
    "    np.save('rnd_test.npy', rnd_test)\n",
    "    print('rnd_test shape: ' + str(rnd_test.shape))\n",
    "    \n",
    "    for beat_idx, beat in enumerate(beats_test):\n",
    "        # if np.random.rand() < channel_ratio:\n",
    "        if (beat_idx // 10) % 2 == 0:\n",
    "            selected_channel = beat_idx % 2  # 0과 1을 번갈아 선택\n",
    "        else:\n",
    "            selected_channel = (beat_idx + 1) % 2  # 반대 순서로 선택\n",
    "        # 노이즈 조합도 순차적으로 선택, 주기적으로 변화를 줌 (매 8회 주기)\n",
    "        noise_combination_idx = (beat_idx % 7) + 1  # 1부터 7까지 순차적으로 선택\n",
    "        noise = combined_noise[selected_channel][:, noise_combination_idx]\n",
    "        noise_segment = noise[noise_index:noise_index + samples]\n",
    "        beat_max_value = np.max(beat) - np.min(beat)\n",
    "        noise_max_value = np.max(noise_segment) - np.min(noise_segment)\n",
    "        Ase = noise_max_value / beat_max_value\n",
    "        alpha = rnd_test[beat_idx] / Ase\n",
    "        signal_noise = beat + alpha * noise_segment\n",
    "        sn_test.append(signal_noise)\n",
    "        noise_indices_test.append(noise_combination_idx)  # 노이즈 인덱스 저장\n",
    "        noise_index += samples\n",
    "        if noise_index > (len(noise) - samples):\n",
    "            noise_index = 0\n",
    "    X_train = np.array(sn_train)[valid_train_indices]  # Match noisy and original beats\n",
    "    X_test = np.array(sn_test)[valid_test_indices]\n",
    "\n",
    "    y_train = np.array(beats_train)[valid_train_indices]  # Match noisy and original beats\n",
    "    y_test = np.array(beats_test)[valid_test_indices]\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    y_train = np.expand_dims(y_train, axis=2)\n",
    "\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    y_test = np.expand_dims(y_test, axis=2)\n",
    "\n",
    "    Dataset = [X_train, y_train, X_test, y_test]\n",
    "    print(f\"[INFO] Final shapes -> X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    print('Dataset ready to use.')\n",
    "\n",
    "    return Dataset, valid_train_indices, valid_test_indices, noise_indices_train, noise_indices_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "import glob\n",
    "from scipy.signal import resample_poly\n",
    "import wfdb\n",
    "import math\n",
    "import _pickle as pickle\n",
    "\n",
    "\n",
    "def make_fourier(inputs, n, fs):\n",
    "    \"\"\"\n",
    "    주파수 도메인 정보 추출 및 time-domain과 같은 shape으로 만듦.\n",
    "    \n",
    "    Parameters:\n",
    "    inputs: 입력 신호 (원본 신호, 2D 배열 - (배치 크기, 샘플 수))\n",
    "    n: FFT 샘플 수\n",
    "    fs: 샘플링 주파수 (예: 360 Hz)\n",
    "    \n",
    "    Returns:\n",
    "    주파수 도메인에서 얻은 신호 (FFT), time-domain과 동일한 크기\n",
    "    \"\"\"\n",
    "    T = n / fs\n",
    "    k = np.arange(n)\n",
    "    freq = k / T\n",
    "    freq = freq[range(int(n / 2))]\n",
    "\n",
    "    signal_list = []\n",
    "    for i in range(inputs.shape[0]):\n",
    "        y = inputs[i, :]\n",
    "        Y = fft(y) / n  # FFT 수행 후 정규화\n",
    "        Y = np.abs(Y[range(int(n / 2))])\n",
    "        # Magnitude 값을 두 배로 늘려 time-domain과 동일한 shape으로 맞춤 (512)\n",
    "        Y_full = np.hstack([Y, Y])  # Duplicate to match time-domain size\n",
    "        signal_list.append(Y_full)\n",
    "\n",
    "    return np.asarray(signal_list)\n",
    "\n",
    "def Data_Preparation_with_Fourier(samples, fs=360):\n",
    "    print('Getting the Data ready ...')\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 1234\n",
    "    np.random.seed(seed=seed)\n",
    "\n",
    "    # Load QT Database\n",
    "    with open('data/QTDatabase.pkl', 'rb') as input:\n",
    "        qtdb = pickle.load(input)\n",
    "\n",
    "    print(f\"[INFO] Loaded QTDatabase with {len(qtdb.keys())} signals\")\n",
    "    # Load combined noise\n",
    "    with open('data/CombinedNoise.pkl', 'rb') as input:\n",
    "        combined_noise = pickle.load(input)\n",
    "    print(f\"[INFO] Loaded CombinedNoise with {len(combined_noise)} channels\")\n",
    "\n",
    "    #####################################\n",
    "    # Data split\n",
    "    #####################################\n",
    "    test_set = ['sel123', 'sel233', 'sel302', 'sel307', 'sel820', 'sel853', \n",
    "                'sel16420', 'sel16795', 'sele0106', 'sele0121', 'sel32', 'sel49', \n",
    "                'sel14046', 'sel15814']\n",
    "\n",
    "    beats_train = []\n",
    "    beats_test = []\n",
    "    fourier_train_x = []\n",
    "    fourier_test_x = []\n",
    "    fourier_train_y = []\n",
    "    fourier_test_y = []\n",
    "    valid_train_indices = []  # To keep track of valid indices in training data\n",
    "    valid_test_indices = []   # To keep track of valid indices in test data\n",
    "    # 노이즈 인덱스 저장 리스트\n",
    "    noise_indices_train = []\n",
    "    noise_indices_test = []\n",
    "    sn_train = []\n",
    "    sn_test = []\n",
    "    \n",
    "    skip_beats = 0\n",
    "    qtdb_keys = list(qtdb.keys())\n",
    "\n",
    "    print(f\"[INFO] Processing QTDatabase, {len(qtdb.keys())} signals to process.\")\n",
    "\n",
    "    for signal_name in qtdb_keys:\n",
    "        for b_idx, b in enumerate(qtdb[signal_name]):\n",
    "            b_np = np.zeros(samples)\n",
    "            b_sq = np.array(b)\n",
    "\n",
    "            init_padding = 16\n",
    "            if b_sq.shape[0] > (samples - init_padding):\n",
    "                skip_beats += 1\n",
    "                continue\n",
    "\n",
    "            b_np[init_padding:b_sq.shape[0] + init_padding] = b_sq - (b_sq[0] + b_sq[-1]) / 2\n",
    "\n",
    "            # Fourier 변환 적용 (주파수 도메인 정보, time-domain과 동일한 shape으로)\n",
    "            fourier_transformed_y = make_fourier(b_np.reshape(1, -1), samples, fs)\n",
    "\n",
    "            if signal_name in test_set:\n",
    "                beats_test.append(b_np)\n",
    "                fourier_test_y.append(fourier_transformed_y[0])  # Append the single batch\n",
    "                valid_test_indices.append(len(beats_test) - 1)  # Track valid test beat index\n",
    "            else:\n",
    "                beats_train.append(b_np)\n",
    "                fourier_train_y.append(fourier_transformed_y[0])  # Append the single batch\n",
    "                valid_train_indices.append(len(beats_train) - 1)  # Track valid train beat index\n",
    "\n",
    "        print(f\"[DEBUG] Processed signal {signal_name}, total beats in train: {len(beats_train)}, total beats in test: {len(beats_test)}\")\n",
    "\n",
    "    #####################################\n",
    "    # Adding noise to train and test sets\n",
    "    #####################################\n",
    "    print(f\"[INFO] Adding noise to train and test sets\")\n",
    "    # Random scaling factor for train and test\n",
    "    rnd_train = np.random.randint(low=20, high=200, size=len(beats_train)) / 100\n",
    "    noise_index = 0\n",
    "    # To ensure equal selection of channels\n",
    "    # Adding noise to train\n",
    "    for beat_idx, beat in enumerate(beats_train):\n",
    "        # if np.random.rand() < channel_ratio:\n",
    "        if (beat_idx // 10) % 2 == 0:\n",
    "            selected_channel = beat_idx % 2  # 0과 1을 번갈아 선택\n",
    "        else:\n",
    "            selected_channel = (beat_idx + 1) % 2  # 반대 순서로 선택\n",
    "\n",
    "        # 노이즈 조합도 순차적으로 선택, 주기적으로 변화를 줌 (매 8회 주기)\n",
    "        noise_combination_idx = (beat_idx % 7) + 1  # 1부터 7까지 순차적으로 선택\n",
    "        noise = combined_noise[selected_channel][:, noise_combination_idx]\n",
    "        noise_segment = noise[noise_index:noise_index + samples]\n",
    "        beat_max_value = np.max(beat) - np.min(beat)\n",
    "        noise_max_value = np.max(noise_segment) - np.min(noise_segment)\n",
    "        Ase = noise_max_value / beat_max_value\n",
    "        alpha = rnd_train[beat_idx] / Ase\n",
    "        signal_noise = beat + alpha * noise_segment\n",
    "        sn_train.append(signal_noise)\n",
    "        fourier_transformed_x = make_fourier(signal_noise.reshape(1, -1), samples, fs)  # X에 대한 Fourier 변환\n",
    "        fourier_train_x.append(fourier_transformed_x[0])  # Append the single batch\n",
    "        noise_indices_train.append(noise_combination_idx)  # 노이즈 인덱스 저장\n",
    "        noise_index += samples\n",
    "        if noise_index > (len(noise) - samples):\n",
    "            noise_index = 0\n",
    "\n",
    "    # Adding noise to test\n",
    "    noise_index = 0\n",
    "    rnd_test = np.random.randint(low=20, high=200, size=len(beats_test)) / 100\n",
    "    np.save('rnd_test.npy', rnd_test)\n",
    "    print('rnd_test shape: ' + str(rnd_test.shape))\n",
    "        \n",
    "    for beat_idx, beat in enumerate(beats_test):\n",
    "        # if np.random.rand() < channel_ratio:\n",
    "        if (beat_idx // 10) % 2 == 0:\n",
    "            selected_channel = beat_idx % 2  # 0과 1을 번갈아 선택\n",
    "        else:\n",
    "            selected_channel = (beat_idx + 1) % 2  # 반대 순서로 선택\n",
    "        # 노이즈 조합도 순차적으로 선택, 주기적으로 변화를 줌 (매 8회 주기)\n",
    "        noise_combination_idx = (beat_idx % 7) + 1  # 1부터 7까지 순차적으로 선택\n",
    "        noise = combined_noise[selected_channel][:, noise_combination_idx]\n",
    "        noise_segment = noise[noise_index:noise_index + samples]\n",
    "        beat_max_value = np.max(beat) - np.min(beat)\n",
    "        noise_max_value = np.max(noise_segment) - np.min(noise_segment)\n",
    "        Ase = noise_max_value / beat_max_value\n",
    "        alpha = rnd_test[beat_idx] / Ase\n",
    "        signal_noise = beat + alpha * noise_segment\n",
    "        sn_test.append(signal_noise)\n",
    "        fourier_transformed_x = make_fourier(signal_noise.reshape(1, -1), samples, fs)  # X에 대한 Fourier 변환\n",
    "        fourier_test_x.append(fourier_transformed_x[0])  # Append the single batch\n",
    "        noise_indices_test.append(noise_combination_idx)  # 노이즈 인덱스 저장\n",
    "        noise_index += samples\n",
    "        if noise_index > (len(noise) - samples):\n",
    "            noise_index = 0\n",
    "\n",
    "\n",
    "    X_train = np.array(sn_train)[valid_train_indices]  # Match noisy and original beats\n",
    "    X_test = np.array(sn_test)[valid_test_indices]\n",
    "\n",
    "    y_train = np.array(beats_train)[valid_train_indices]  # Match noisy and original beats\n",
    "    y_test = np.array(beats_test)[valid_test_indices]\n",
    "\n",
    "    # Fourier 정보도 포함된 주파수 도메인 데이터셋 생성\n",
    "    F_train_x = np.array(fourier_train_x)[valid_train_indices]\n",
    "    F_test_x = np.array(fourier_test_x)[valid_test_indices]\n",
    "    F_train_y = np.array(fourier_train_y)[valid_train_indices]\n",
    "    F_test_y = np.array(fourier_test_y)[valid_test_indices]\n",
    "\n",
    "    # Shape을 time-domain과 동일하게 확장\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    y_train = np.expand_dims(y_train, axis=2)\n",
    "\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    y_test = np.expand_dims(y_test, axis=2)\n",
    "\n",
    "    F_train_x = np.expand_dims(F_train_x, axis=2)\n",
    "    F_train_y = np.expand_dims(F_train_y, axis=2)\n",
    "    \n",
    "    F_test_x = np.expand_dims(F_test_x, axis=2)\n",
    "    F_test_y = np.expand_dims(F_test_y, axis=2)\n",
    "\n",
    "    Dataset = [X_train, y_train, X_test, y_test, F_train_x, F_train_y, F_test_x, F_test_y]\n",
    "    \n",
    "    print(f\"[INFO] Final shapes -> X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    print(f\"[INFO] Fourier shapes -> F_train_x: {F_train_x.shape}, F_train_y: {F_train_y.shape}, F_test_x: {F_test_x.shape}, F_test_y: {F_test_y.shape}\")\n",
    "    print('Dataset ready to use.')\n",
    "\n",
    "    return Dataset, valid_train_indices, valid_test_indices, noise_indices_train, noise_indices_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Data_Preparation_with_Fourier() got an unexpected keyword argument 'channel_ratio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35948/3463741081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Unpack the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_train_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_test_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_indices_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_indices_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData_Preparation_with_Fourier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m360\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_test_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print dataset shapes for confirmation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Data_Preparation_with_Fourier() got an unexpected keyword argument 'channel_ratio'"
     ]
    }
   ],
   "source": [
    "# Unpack the dataset\n",
    "Dataset, valid_train_indices, valid_test_indices, noise_indices_train, noise_indices_test = Data_Preparation_with_Fourier(samples=512, fs=360)\n",
    "X_train, y_train, X_test, y_test, F_train_x, F_train_y, F_test_x, F_test_y = Dataset\n",
    "\n",
    "# Print dataset shapes for confirmation\n",
    "print(f\"Time domain train shapes: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Frequency domain train shapes: F_train_x: {F_train_x.shape}, F_train_y: {F_train_y.shape}\")\n",
    "print(f\"Time domain test shapes: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"Frequency domain test shapes: F_test_x: {F_test_x.shape}, F_test_y: {F_test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import _pickle as pickle\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "dl_experiments = ['DRNN',\n",
    "                  'FCN-DAE',\n",
    "                  'Vanilla L',\n",
    "                  'Vanilla NL',\n",
    "                  'Multibranch LANL',\n",
    "                  'Multibranch LANLD',\n",
    "                  'Transformer_DAE',\n",
    "                  # 'Transformer_FDAE',\n",
    "                #   'Transformer_FREDAE',\n",
    "                  'Transformer_COMBDAE',\n",
    "                  # 'Transformer_COMBDAE_with_CrossDomainAttention',\n",
    "                  # 'Transformer_COMBDAE_FreTS'\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_date = datetime.now().strftime('%m%d')\n",
    "# train_time_list = []\n",
    "# test_time_list = []\n",
    "\n",
    "# # Classical Filters\n",
    "\n",
    "# # FIR\n",
    "# print('Running FIR fiter on the test set. This will take a while (2h)...')\n",
    "# start_test = datetime.now()\n",
    "# [X_test_f, y_test_f, y_filter] = FIR_test_Dataset(Dataset)\n",
    "# end_test = datetime.now()\n",
    "# train_time_list.append(0)\n",
    "# test_time_list.append(end_test - start_test)\n",
    "\n",
    "# test_results_FIR = [X_test_f, y_test_f, y_filter]\n",
    "\n",
    "# # Save FIR filter results\n",
    "# with open('test_results_FIR.pkl', 'wb') as output:  # Overwrites any existing file.\n",
    "#     pickle.dump(test_results_FIR, output)\n",
    "# print('Results from experiment FIR filter saved')\n",
    "\n",
    "# # IIR\n",
    "# print('Running IIR fiter on the test set. This will take a while (25 mins)...')\n",
    "# start_test = datetime.now()\n",
    "# [X_test_f, y_test_f, y_filter] = IIR_test_Dataset(Dataset)\n",
    "# end_test = datetime.now()\n",
    "# train_time_list.append(0)\n",
    "# test_time_list.append(end_test - start_test)\n",
    "\n",
    "# test_results_IIR = [X_test_f, y_test_f, y_filter]\n",
    "\n",
    "# # Save IIR filter results\n",
    "# with open('test_results_IIR.pkl', 'wb') as output:  # Overwrites any existing file.\n",
    "#     pickle.dump(test_results_IIR, output)\n",
    "# print('Results from experiment IIR filter saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Metric functions\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def SSD(y, y_pred):\n",
    "    return np.sum(np.square(y - y_pred), axis=1)  # axis 1 is the signal dimension\n",
    "\n",
    "\n",
    "def MAD(y, y_pred):\n",
    "    return np.max(np.abs(y - y_pred), axis=1) # axis 1 is the signal dimension\n",
    "\n",
    "\n",
    "def PRD(y, y_pred):\n",
    "    N = np.sum(np.square(y_pred - y), axis=1)\n",
    "    D = np.sum(np.square(y_pred - np.mean(y)), axis=1)\n",
    "\n",
    "    PRD = np.sqrt(N/D) * 100\n",
    "\n",
    "    return PRD\n",
    "\n",
    "\n",
    "def COS_SIM(y, y_pred):\n",
    "    cos_sim = []\n",
    "\n",
    "    y = np.squeeze(y, axis=-1)\n",
    "    y_pred = np.squeeze(y_pred, axis=-1)\n",
    "\n",
    "    for idx in range(len(y)):\n",
    "        kl_temp = cosine_similarity(y[idx].reshape(1, -1), y_pred[idx].reshape(1, -1))\n",
    "        cos_sim.append(kl_temp)\n",
    "\n",
    "    cos_sim = np.array(cos_sim)\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving timing list\n",
    "# timing = [train_time_list, test_time_list]\n",
    "# with open('timing.pkl', 'wb') as output:  # Overwrites any existing file.\n",
    "#     pickle.dump(timing, output)\n",
    "# print('Timing saved')\n",
    "\n",
    "# # Load timing\n",
    "# with open('timing.pkl', 'rb') as input:\n",
    "#     timing = pickle.load(input)\n",
    "#     [train_time_list, test_time_list] = timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def generate_table(metrics, metric_values, Exp_names):\n",
    "    # Print tabular results in the console, in a pretty way\n",
    "    print('\\n')\n",
    "\n",
    "    tb = PrettyTable()\n",
    "    ind = 0\n",
    "\n",
    "    for exp_name in Exp_names:\n",
    "\n",
    "        tb.field_names = ['Method/Model'] + metrics\n",
    "\n",
    "        tb_row = []\n",
    "        tb_row.append(exp_name)\n",
    "\n",
    "        for metric in metric_values:   # metric_values[metric][model][beat]\n",
    "            m_mean = np.mean(metric[ind])\n",
    "            m_std = np.std(metric[ind])\n",
    "            tb_row.append('{:.3f}'.format(m_mean) + ' (' + '{:.3f}'.format(m_std) + ')')\n",
    "\n",
    "        tb.add_row(tb_row)\n",
    "        ind += 1\n",
    "\n",
    "    print(tb)\n",
    "\n",
    "def generate_table_time(column_names, all_values, Exp_names, gpu=True):\n",
    "    # Print tabular results in the console, in a pretty way\n",
    "\n",
    "    # The FIR and IIR are the last on all_values\n",
    "    # We need circular shift them to the right\n",
    "    all_values[0] = all_values[0][-2::] + all_values[0][0:-2]\n",
    "    all_values[1] = all_values[1][-2::] + all_values[1][0:-2]\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    tb = PrettyTable()\n",
    "    ind = 0\n",
    "\n",
    "    if gpu:\n",
    "        device = 'GPU'\n",
    "    else:\n",
    "        device = 'CPU'\n",
    "\n",
    "    for exp_name in Exp_names:\n",
    "        tb.field_names = ['Method/Model'] + [column_names[0] + '(' + device + ') h:m:s:ms'] + [\n",
    "            column_names[1] + '(' + device + ') h:m:s:ms']\n",
    "\n",
    "        tb_row = []\n",
    "        tb_row.append(exp_name)\n",
    "        tb_row.append(all_values[0][ind])\n",
    "        tb_row.append(all_values[1][ind])\n",
    "\n",
    "        tb.add_row(tb_row)\n",
    "\n",
    "        ind += 1\n",
    "\n",
    "    print(tb)\n",
    "\n",
    "    if gpu:\n",
    "        print('* For FIR and IIR Filters is CPU since scipy filters are CPU based implementations')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics ...\n",
      "\n",
      "\n",
      "+---------------------------+------------------+---------------+------------------+---------------+\n",
      "|        Method/Model       |       SSD        |      MAD      |       PRD        |    COS_SIM    |\n",
      "+---------------------------+------------------+---------------+------------------+---------------+\n",
      "|         FIR Filter        | 95.588 (159.388) | 1.067 (0.834) | 76.842 (19.781)  | 0.574 (0.235) |\n",
      "|         IIR Filter        | 88.049 (143.279) | 1.051 (0.847) | 75.632 (20.763)  | 0.586 (0.237) |\n",
      "|            DRNN           |  7.293 (9.894)   | 0.548 (0.380) | 63.972 (40.631)  | 0.861 (0.139) |\n",
      "|          FCN-DAE          | 10.567 (15.440)  | 0.621 (0.418) | 83.941 (62.513)  | 0.781 (0.204) |\n",
      "|         Vanilla L         | 22.038 (25.212)  | 0.753 (0.385) | 107.717 (33.303) | 0.594 (0.179) |\n",
      "|         Vanilla NL        | 11.456 (14.291)  | 0.582 (0.367) | 96.566 (61.925)  | 0.759 (0.181) |\n",
      "|      Multibranch LANL     |  9.836 (12.276)  | 0.515 (0.337) | 81.932 (47.026)  | 0.795 (0.164) |\n",
      "|     Multibranch LANLD     |  7.981 (10.874)  | 0.461 (0.330) | 67.068 (40.303)  | 0.837 (0.151) |\n",
      "|      Transformer_DAE      |  5.823 (8.508)   | 0.395 (0.282) | 48.457 (26.317)  | 0.892 (0.118) |\n",
      "|    Transformer_COMBDAE    |  5.856 (8.248)   | 0.404 (0.290) | 49.130 (25.810)  | 0.893 (0.111) |\n",
      "| Transformer_COMBDAE_FreTS |  5.840 (8.187)   | 0.410 (0.280) | 49.650 (26.336)  | 0.890 (0.113) |\n",
      "+---------------------------+------------------+---------------+------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "####### LOAD EXPERIMENTS #######\n",
    "\n",
    "# Load Results DRNN\n",
    "with open('1006/test_results_' + dl_experiments[0] + '.pkl', 'rb') as input:\n",
    "    test_DRNN = pickle.load(input)\n",
    "\n",
    "# Load Results FCN_DAE\n",
    "with open('1006/test_results_' + dl_experiments[1] + '.pkl', 'rb') as input:\n",
    "    test_FCN_DAE = pickle.load(input)\n",
    "\n",
    "# Load Results Vanilla L\n",
    "with open('1006/test_results_' + dl_experiments[2] + '.pkl', 'rb') as input:\n",
    "    test_Vanilla_L = pickle.load(input)\n",
    "\n",
    "# Load Results Exp Vanilla NL\n",
    "with open('1006/test_results_' + dl_experiments[3] + '.pkl', 'rb') as input:\n",
    "    test_Vanilla_NL = pickle.load(input)\n",
    "\n",
    "# Load Results Multibranch LANL\n",
    "with open('1006/test_results_' + dl_experiments[4] + '.pkl', 'rb') as input:\n",
    "    test_Multibranch_LANL = pickle.load(input)\n",
    "\n",
    "# Load Results Multibranch LANLD\n",
    "with open('1006/test_results_' + dl_experiments[5] + '.pkl', 'rb') as input:\n",
    "    test_Multibranch_LANLD = pickle.load(input)\n",
    "# Load Results Transformer_DAE\n",
    "\n",
    "with open('1006/test_results_' + dl_experiments[6] + '.pkl', 'rb') as input:\n",
    "    test_Transformer_DAE = pickle.load(input)\n",
    "\n",
    "# # Load Results Transformer_FDAE\n",
    "# with open('0920/test_results_' + dl_experiments[7] + '.pkl', 'rb') as input:\n",
    "#     test_Transformer_FDAE = pickle.load(input)\n",
    "    # Transformer_COMBDAE_with_CrossDomainAttention\n",
    "# Load Results Transformer_FDAE\n",
    "with open('1006/test_results_' + dl_experiments[7] + '.pkl', 'rb') as input:\n",
    "    test_Transformer_COMBDAE = pickle.load(input)\n",
    "        \n",
    "with open('1004/test_results_' + dl_experiments[8] + '.pkl', 'rb') as input:\n",
    "    test_Transformer_COMBDAE_FreTS = pickle.load(input)   \n",
    "    \n",
    "    \n",
    "# Load Result FIR Filter\n",
    "with open('1005/test_results_FIR.pkl', 'rb') as input:\n",
    "    test_FIR = pickle.load(input)\n",
    "\n",
    "# Load Result IIR Filter\n",
    "with open('1005/test_results_IIR.pkl', 'rb') as input:\n",
    "    test_IIR = pickle.load(input)\n",
    "\n",
    "####### Calculate Metrics #######\n",
    "\n",
    "print('Calculating metrics ...')\n",
    "\n",
    "# DL Metrics\n",
    "\n",
    "# Exp FCN-DAE\n",
    "\n",
    "[X_test, y_test, y_pred] = test_DRNN\n",
    "\n",
    "SSD_values_DL_DRNN = SSD(y_test, y_pred)\n",
    "\n",
    "MAD_values_DL_DRNN = MAD(y_test, y_pred)\n",
    "\n",
    "PRD_values_DL_DRNN = PRD(y_test, y_pred)\n",
    "\n",
    "COS_SIM_values_DL_DRNN = COS_SIM(y_test, y_pred)\n",
    "\n",
    "# Exp FCN-DAE\n",
    "\n",
    "[X_test, y_test, y_pred] = test_FCN_DAE\n",
    "\n",
    "SSD_values_DL_FCN_DAE = SSD(y_test, y_pred)\n",
    "\n",
    "MAD_values_DL_FCN_DAE = MAD(y_test, y_pred)\n",
    "\n",
    "PRD_values_DL_FCN_DAE = PRD(y_test, y_pred)\n",
    "\n",
    "COS_SIM_values_DL_FCN_DAE = COS_SIM(y_test, y_pred)\n",
    "\n",
    "# Vanilla L\n",
    "\n",
    "[X_test, y_test, y_pred] = test_Vanilla_L\n",
    "\n",
    "SSD_values_DL_exp_1 = SSD(y_test, y_pred)\n",
    "\n",
    "MAD_values_DL_exp_1 = MAD(y_test, y_pred)\n",
    "\n",
    "PRD_values_DL_exp_1 = PRD(y_test, y_pred)\n",
    "\n",
    "COS_SIM_values_DL_exp_1 = COS_SIM(y_test, y_pred)\n",
    "\n",
    "# Vanilla_NL\n",
    "\n",
    "[X_test, y_test, y_pred] = test_Vanilla_NL\n",
    "\n",
    "SSD_values_DL_exp_2 = SSD(y_test, y_pred)\n",
    "\n",
    "MAD_values_DL_exp_2 = MAD(y_test, y_pred)\n",
    "\n",
    "PRD_values_DL_exp_2 = PRD(y_test, y_pred)\n",
    "\n",
    "COS_SIM_values_DL_exp_2 = COS_SIM(y_test, y_pred)\n",
    "\n",
    "# Multibranch_LANL\n",
    "\n",
    "[X_test, y_test, y_pred] = test_Multibranch_LANL\n",
    "\n",
    "SSD_values_DL_exp_3 = SSD(y_test, y_pred)\n",
    "\n",
    "MAD_values_DL_exp_3 = MAD(y_test, y_pred)\n",
    "\n",
    "PRD_values_DL_exp_3 = PRD(y_test, y_pred)\n",
    "\n",
    "COS_SIM_values_DL_exp_3 = COS_SIM(y_test, y_pred)\n",
    "\n",
    "# Multibranch_LANLD\n",
    "\n",
    "[X_test, y_test, y_pred] = test_Multibranch_LANLD\n",
    "\n",
    "SSD_values_DL_exp_4 = SSD(y_test, y_pred)\n",
    "\n",
    "MAD_values_DL_exp_4 = MAD(y_test, y_pred)\n",
    "\n",
    "PRD_values_DL_exp_4 = PRD(y_test, y_pred)\n",
    "\n",
    "COS_SIM_values_DL_exp_4 = COS_SIM(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Transformer_DAE\n",
    "\n",
    "[X_test, y_test, y_pred] = test_Transformer_DAE\n",
    "\n",
    "SSD_values_DL_exp_5 = SSD(y_test, y_pred)\n",
    "\n",
    "MAD_values_DL_exp_5 = MAD(y_test, y_pred)\n",
    "\n",
    "PRD_values_DL_exp_5 = PRD(y_test, y_pred)\n",
    "\n",
    "COS_SIM_values_DL_exp_5 = COS_SIM(y_test, y_pred)\n",
    "\n",
    "# Transformer_FDAE\n",
    "\n",
    "[X_test, y_test, y_pred] = test_Transformer_COMBDAE\n",
    "\n",
    "SSD_values_DL_exp_6 = SSD(y_test, y_pred)\n",
    "\n",
    "MAD_values_DL_exp_6 = MAD(y_test, y_pred)\n",
    "\n",
    "PRD_values_DL_exp_6 = PRD(y_test, y_pred)\n",
    "\n",
    "COS_SIM_values_DL_exp_6 = COS_SIM(y_test, y_pred)\n",
    "\n",
    "\n",
    "# # Transformer_FDAE\n",
    "\n",
    "[X_test, y_test, y_pred] = test_Transformer_COMBDAE_FreTS\n",
    "\n",
    "SSD_values_DL_exp_7 = SSD(y_test, y_pred)\n",
    "\n",
    "MAD_values_DL_exp_7 = MAD(y_test, y_pred)\n",
    "\n",
    "PRD_values_DL_exp_7 = PRD(y_test, y_pred)\n",
    "\n",
    "COS_SIM_values_DL_exp_7 = COS_SIM(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Digital Filtering\n",
    "\n",
    "# FIR Filtering Metrics\n",
    "[X_test, y_test, y_filter] = test_FIR\n",
    "\n",
    "SSD_values_FIR = SSD(y_test, y_filter)\n",
    "\n",
    "MAD_values_FIR = MAD(y_test, y_filter)\n",
    "\n",
    "PRD_values_FIR = PRD(y_test, y_filter)\n",
    "\n",
    "COS_SIM_values_FIR = COS_SIM(y_test, y_filter)\n",
    "\n",
    "# IIR Filtering Metrics (Best)\n",
    "[X_test, y_test, y_filter] = test_IIR\n",
    "\n",
    "SSD_values_IIR = SSD(y_test, y_filter)\n",
    "\n",
    "MAD_values_IIR = MAD(y_test, y_filter)\n",
    "\n",
    "PRD_values_IIR = PRD(y_test, y_filter)\n",
    "\n",
    "COS_SIM_values_IIR = COS_SIM(y_test, y_filter)\n",
    "\n",
    "####### Results Visualization #######\n",
    "\n",
    "SSD_all = [SSD_values_FIR,\n",
    "           SSD_values_IIR,\n",
    "           SSD_values_DL_FCN_DAE,\n",
    "           SSD_values_DL_DRNN,\n",
    "           SSD_values_DL_exp_1,\n",
    "           SSD_values_DL_exp_2,\n",
    "           SSD_values_DL_exp_3,\n",
    "           SSD_values_DL_exp_4,\n",
    "            SSD_values_DL_exp_5,\n",
    "           SSD_values_DL_exp_6,\n",
    "           SSD_values_DL_exp_7\n",
    "           ]\n",
    "\n",
    "MAD_all = [MAD_values_FIR,\n",
    "           MAD_values_IIR,\n",
    "           MAD_values_DL_FCN_DAE,\n",
    "           MAD_values_DL_DRNN,\n",
    "           MAD_values_DL_exp_1,\n",
    "           MAD_values_DL_exp_2,\n",
    "           MAD_values_DL_exp_3,\n",
    "           MAD_values_DL_exp_4,\n",
    "           MAD_values_DL_exp_5,\n",
    "           MAD_values_DL_exp_6,\n",
    "           MAD_values_DL_exp_7\n",
    "           ]\n",
    "\n",
    "PRD_all = [PRD_values_FIR,\n",
    "           PRD_values_IIR,\n",
    "           PRD_values_DL_FCN_DAE,\n",
    "           PRD_values_DL_DRNN,\n",
    "           PRD_values_DL_exp_1,\n",
    "           PRD_values_DL_exp_2,\n",
    "           PRD_values_DL_exp_3,\n",
    "           PRD_values_DL_exp_4,\n",
    "           PRD_values_DL_exp_5,\n",
    "           PRD_values_DL_exp_6,\n",
    "           PRD_values_DL_exp_7\n",
    "           ]\n",
    "\n",
    "CORR_all = [COS_SIM_values_FIR,\n",
    "            COS_SIM_values_IIR,\n",
    "            COS_SIM_values_DL_FCN_DAE,\n",
    "            COS_SIM_values_DL_DRNN,\n",
    "            COS_SIM_values_DL_exp_1,\n",
    "            COS_SIM_values_DL_exp_2,\n",
    "            COS_SIM_values_DL_exp_3,\n",
    "            COS_SIM_values_DL_exp_4,\n",
    "            COS_SIM_values_DL_exp_5,\n",
    "            COS_SIM_values_DL_exp_6,\n",
    "            COS_SIM_values_DL_exp_7\n",
    "            ]\n",
    "\n",
    "Exp_names = ['FIR Filter', 'IIR Filter'] + dl_experiments\n",
    "\n",
    "metrics = ['SSD', 'MAD', 'PRD', 'COS_SIM']\n",
    "metric_values = [SSD_all, MAD_all, PRD_all, CORR_all]\n",
    "\n",
    "# Metrics table\n",
    "generate_table(metrics, metric_values, Exp_names)\n",
    "\n",
    "# # Timing table\n",
    "# timing_var = ['training', 'test']\n",
    "# generate_table_time(timing_var, timing, Exp_names, gpu=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 13316, 512, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_DRNN).shape\n",
    "# # np.array(test_Transformer_COMBDAE_FreTS).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SSD plots...\n",
      "Saved: plots/ssd_hboxplot.png\n",
      "Saved: plots/ssd_violinplot.png\n",
      "Saved: plots/ssd_barplot.png\n",
      "Saved: plots/ssd_boxplot.png\n",
      "Generating MAD plots...\n",
      "Saved: plots/mad_hboxplot.png\n",
      "Saved: plots/mad_violinplot.png\n",
      "Saved: plots/mad_barplot.png\n",
      "Saved: plots/mad_boxplot.png\n",
      "Generating PRD plots...\n",
      "Saved: plots/prd_hboxplot.png\n",
      "Saved: plots/prd_violinplot.png\n",
      "Saved: plots/prd_barplot.png\n",
      "Saved: plots/prd_boxplot.png\n",
      "Generating Cosine Similarity plots...\n",
      "Saved: plots/cos_hboxplot.png\n",
      "Saved: plots/cos_violinplot.png\n",
      "Saved: plots/cos_barplot.png\n",
      "Saved: plots/cos_boxplot.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ensure_directory(directory):\n",
    "    \"\"\" 디렉토리가 존재하지 않으면 생성 \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def generate_hboxplot(np_data, description, ylabel, log, save_dir, filename, set_x_axis_size=None):\n",
    "    # Process the results and store in Pandas DataFrame\n",
    "    ensure_directory(save_dir)  # 디렉토리 생성\n",
    "    col = description\n",
    "    loss_val_np = np.rot90(np_data)\n",
    "    pd_df = pd.DataFrame.from_records(loss_val_np, columns=col)\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    f, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "    ax = sns.boxplot(data=pd_df, orient=\"h\", width=0.4)  # 가로로 누운 boxplot\n",
    "\n",
    "    if log:\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "    if set_x_axis_size is not None:\n",
    "        ax.set_xlim(set_x_axis_size)\n",
    "\n",
    "    ax.set(ylabel='Models/Methods', xlabel=ylabel)\n",
    "    ax = sns.despine(left=True, bottom=True)\n",
    "\n",
    "    # Save plot to file\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filepath}\")\n",
    "\n",
    "\n",
    "def generate_violinplots(np_data, description, ylabel, log, save_dir, filename, set_x_axis_size=None):\n",
    "    # Process the results and store in Pandas DataFrame\n",
    "    ensure_directory(save_dir)  # 디렉토리 생성\n",
    "    col = description\n",
    "    loss_val_np = np.rot90(np_data)\n",
    "    pd_df = pd.DataFrame.from_records(loss_val_np, columns=col)\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(15, 6))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    ax = sns.violinplot(data=pd_df, palette=\"Set3\", bw=.2, cut=1, linewidth=1, orient=\"h\")  # 가로로 누운 violinplot\n",
    "\n",
    "    if log:\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "    if set_x_axis_size is not None:\n",
    "        ax.set_xlim(set_x_axis_size)\n",
    "\n",
    "    ax.set(xlabel='Models/Methods', ylabel=ylabel)\n",
    "    ax = sns.despine(left=True, bottom=True)\n",
    "\n",
    "    # Save plot to file\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filepath}\")\n",
    "\n",
    "\n",
    "def generate_barplot(np_data, description, ylabel, log, save_dir, filename, set_x_axis_size=None):\n",
    "    # Process the results and store in Pandas DataFrame\n",
    "    ensure_directory(save_dir)  # 디렉토리 생성\n",
    "    col = description\n",
    "    loss_val_np = np.rot90(np_data)\n",
    "    pd_df = pd.DataFrame.from_records(loss_val_np, columns=col)\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(15, 6))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    ax = sns.barplot(data=pd_df, orient=\"h\")  # 가로로 누운 barplot\n",
    "\n",
    "    if log:\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "    if set_x_axis_size is not None:\n",
    "        ax.set_xlim(set_x_axis_size)\n",
    "\n",
    "    ax.set(xlabel='Models/Methods', ylabel=ylabel)\n",
    "    ax = sns.despine(left=True, bottom=True)\n",
    "\n",
    "    # Save plot to file\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filepath}\")\n",
    "\n",
    "\n",
    "def generate_boxplot(np_data, description, ylabel, log, save_dir, filename, set_x_axis_size=None):\n",
    "    # Process the results and store in Pandas DataFrame\n",
    "    ensure_directory(save_dir)  # 디렉토리 생성\n",
    "    col = description\n",
    "    loss_val_np = np.rot90(np_data)\n",
    "    pd_df = pd.DataFrame.from_records(loss_val_np, columns=col)\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(15, 6))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    ax = sns.boxplot(data=pd_df, orient=\"h\")  # 가로로 누운 boxplot\n",
    "\n",
    "    if log:\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "    if set_x_axis_size is not None:\n",
    "        ax.set_xlim(set_x_axis_size)\n",
    "\n",
    "    ax.set(xlabel='Models/Methods', ylabel=ylabel)\n",
    "    ax = sns.despine(left=True, bottom=True)\n",
    "\n",
    "    # Save plot to file\n",
    "    filepath = os.path.join(save_dir, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filepath}\")\n",
    "\n",
    "\n",
    "# 저장 경로 설정\n",
    "save_directory = 'plots'\n",
    "\n",
    "# 파일 이름 설정\n",
    "filename_ssd_hbox = 'ssd_hboxplot.png'\n",
    "filename_ssd_violin = 'ssd_violinplot.png'\n",
    "filename_ssd_bar = 'ssd_barplot.png'\n",
    "filename_ssd_box = 'ssd_boxplot.png'\n",
    "\n",
    "filename_mad_hbox = 'mad_hboxplot.png'\n",
    "filename_mad_violin = 'mad_violinplot.png'\n",
    "filename_mad_bar = 'mad_barplot.png'\n",
    "filename_mad_box = 'mad_boxplot.png'\n",
    "\n",
    "filename_prd_hbox = 'prd_hboxplot.png'\n",
    "filename_prd_violin = 'prd_violinplot.png'\n",
    "filename_prd_bar = 'prd_barplot.png'\n",
    "filename_prd_box = 'prd_boxplot.png'\n",
    "\n",
    "filename_cos_hbox = 'cos_hboxplot.png'\n",
    "filename_cos_violin = 'cos_violinplot.png'\n",
    "filename_cos_bar = 'cos_barplot.png'\n",
    "filename_cos_box = 'cos_boxplot.png'\n",
    "\n",
    "# SSD 그래프들 생성\n",
    "print(\"Generating SSD plots...\")\n",
    "generate_hboxplot(SSD_all, Exp_names, 'SSD (au)', log=False, save_dir=save_directory, filename=filename_ssd_hbox, set_x_axis_size=(0, 100.1))\n",
    "generate_violinplots(SSD_all, Exp_names, 'SSD (au)', log=False, save_dir=save_directory, filename=filename_ssd_violin, set_x_axis_size=(0, 100.1))\n",
    "generate_barplot(SSD_all, Exp_names, 'SSD (au)', log=False, save_dir=save_directory, filename=filename_ssd_bar, set_x_axis_size=(0, 100.1))\n",
    "generate_boxplot(SSD_all, Exp_names, 'SSD (au)', log=False, save_dir=save_directory, filename=filename_ssd_box, set_x_axis_size=(0, 100.1))\n",
    "\n",
    "# MAD 그래프들 생성\n",
    "print(\"Generating MAD plots...\")\n",
    "generate_hboxplot(MAD_all, Exp_names, 'MAD (au)', log=False, save_dir=save_directory, filename=filename_mad_hbox, set_x_axis_size=(0, 3.01))\n",
    "generate_violinplots(MAD_all, Exp_names, 'MAD (au)', log=False, save_dir=save_directory, filename=filename_mad_violin, set_x_axis_size=(0, 3.01))\n",
    "generate_barplot(MAD_all, Exp_names, 'MAD (au)', log=False, save_dir=save_directory, filename=filename_mad_bar, set_x_axis_size=(0, 3.01))\n",
    "generate_boxplot(MAD_all, Exp_names, 'MAD (au)', log=False, save_dir=save_directory, filename=filename_mad_box, set_x_axis_size=(0, 3.01))\n",
    "\n",
    "# PRD 그래프들 생성\n",
    "print(\"Generating PRD plots...\")\n",
    "generate_hboxplot(PRD_all, Exp_names, 'PRD (au)', log=False, save_dir=save_directory, filename=filename_prd_hbox, set_x_axis_size=(0, 150.1))\n",
    "generate_violinplots(PRD_all, Exp_names, 'PRD (au)', log=False, save_dir=save_directory, filename=filename_prd_violin, set_x_axis_size=(0, 150.1))\n",
    "generate_barplot(PRD_all, Exp_names, 'PRD (au)', log=False, save_dir=save_directory, filename=filename_prd_bar, set_x_axis_size=(0, 150.1))\n",
    "generate_boxplot(PRD_all, Exp_names, 'PRD (au)', log=False, save_dir=save_directory, filename=filename_prd_box, set_x_axis_size=(0, 150.1))\n",
    "\n",
    "# Cosine Similarity 그래프들 생성\n",
    "print(\"Generating Cosine Similarity plots...\")\n",
    "generate_hboxplot(CORR_all, Exp_names, 'Cosine Similarity (0-1)', log=False, save_dir=save_directory, filename=filename_cos_hbox, set_x_axis_size=(0, 1))\n",
    "generate_violinplots(CORR_all, Exp_names, 'Cosine Similarity (0-1)', log=False, save_dir=save_directory, filename=filename_cos_violin, set_x_axis_size=(0, 1))\n",
    "generate_barplot(CORR_all, Exp_names, 'Cosine Similarity (0-1)', log=False, save_dir=save_directory, filename=filename_cos_bar, set_x_axis_size=(0, 1))\n",
    "generate_boxplot(CORR_all, Exp_names, 'Cosine Similarity (0-1)', log=False, save_dir=save_directory, filename=filename_cos_box, set_x_axis_size=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def generate_hboxplot(np_data, description, ylabel, log, set_x_axis_size=None):\n",
    "#     # Process the results and store in Panda objects\n",
    "#     col = description\n",
    "#     loss_val_np = np.rot90(np_data)\n",
    "\n",
    "#     pd_df = pd.DataFrame.from_records(loss_val_np, columns=col)\n",
    "\n",
    "#     # Set up the matplotlib figure\n",
    "#     sns.set(style=\"whitegrid\")\n",
    "\n",
    "#     f, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "#     ax = sns.boxplot(data=pd_df, orient=\"h\", width=0.4)\n",
    "\n",
    "#     if log:\n",
    "#         ax.set_xscale(\"log\")\n",
    "\n",
    "#     if set_x_axis_size != None:\n",
    "#         ax.set_xlim(set_x_axis_size)\n",
    "\n",
    "#     ax.set(ylabel='Models/Methods', xlabel=ylabel)\n",
    "#     ax = sns.despine(left=True, bottom=True)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Metrics graphs\n",
    "# print('SSD Metric comparative graph')\n",
    "# generate_hboxplot(SSD_all, Exp_names, 'SSD (au)', log=False, set_x_axis_size=(0, 100.1))\n",
    "# print('MAD Metric comparative graph')\n",
    "# generate_hboxplot(MAD_all, Exp_names, 'MAD (au)', log=False, set_x_axis_size=(0, 3.01))\n",
    "# print('PRD Metric comparative graph')\n",
    "# generate_hboxplot(PRD_all, Exp_names, 'PRD (au)', log=False, set_x_axis_size=(0, 150.1))\n",
    "# print('Cosine Similarity Metric comparative graph')\n",
    "# generate_hboxplot(CORR_all, Exp_names, 'Cosine Similarity (0-1)', log=False, set_x_axis_size=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhc99/anaconda3/envs/DeepFilter/lib/python3.7/site-packages/ipykernel_launcher.py:75: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/dhc99/anaconda3/envs/DeepFilter/lib/python3.7/site-packages/ipykernel_launcher.py:81: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/home/dhc99/anaconda3/envs/DeepFilter/lib/python3.7/site-packages/ipykernel_launcher.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Printing Table for different noise values on the SSD metric\n",
      "\n",
      "\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|        Method/Model       | 0.2 < noise < 0.6 | 0.6 < noise < 1.0 | 1.0 < noise < 1.5 | 1.5 < noise < 2.0 |\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|         FIR Filter        |  12.707 (16.084)  |  43.508 (52.420)  | 102.134 (120.641) | 199.366 (236.760) |\n",
      "|         IIR Filter        |  11.738 (14.964)  |  40.651 (50.495)  |  95.015 (112.642) | 182.173 (209.662) |\n",
      "|            DRNN           |   5.038 (6.624)   |   6.237 (9.049)   |   7.405 (8.984)   |   9.812 (12.342)  |\n",
      "|          FCN-DAE          |   6.447 (8.270)   |   8.485 (10.975)  |  10.877 (14.359)  |  15.213 (20.816)  |\n",
      "|         Vanilla L         |   10.072 (7.180)  |  15.072 (14.794)  |  23.063 (20.751)  |  36.440 (35.379)  |\n",
      "|         Vanilla NL        |   6.226 (6.762)   |   9.438 (12.489)  |  12.146 (12.738)  |  16.606 (17.352)  |\n",
      "|      Multibranch LANL     |   4.462 (4.617)   |   7.645 (10.145)  |  10.612 (10.404)  |  15.196 (15.805)  |\n",
      "|     Multibranch LANLD     |   3.286 (3.827)   |   5.967 (8.621)   |   8.681 (9.567)   |  12.729 (14.338)  |\n",
      "|      Transformer_DAE      |   3.154 (4.390)   |   4.772 (7.189)   |   6.116 (7.744)   |   8.517 (10.957)  |\n",
      "|    Transformer_COMBDAE    |   3.243 (4.308)   |   4.818 (7.180)   |   6.171 (8.000)   |   8.480 (10.505)  |\n",
      "| Transformer_COMBDAE_FreTS |   3.629 (4.797)   |   5.062 (7.156)   |   6.052 (7.848)   |   8.037 (10.517)  |\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "Printing Table for different noise values on the MAD metric\n",
      "\n",
      "\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|        Method/Model       | 0.2 < noise < 0.6 | 0.6 < noise < 1.0 | 1.0 < noise < 1.5 | 1.5 < noise < 2.0 |\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|         FIR Filter        |   0.403 (0.253)   |   0.782 (0.461)   |   1.217 (0.706)   |   1.695 (0.975)   |\n",
      "|         IIR Filter        |   0.394 (0.257)   |   0.771 (0.479)   |   1.202 (0.729)   |   1.671 (1.003)   |\n",
      "|            DRNN           |   0.445 (0.292)   |   0.498 (0.347)   |   0.563 (0.368)   |   0.656 (0.442)   |\n",
      "|          FCN-DAE          |   0.476 (0.307)   |   0.543 (0.347)   |   0.642 (0.395)   |   0.780 (0.502)   |\n",
      "|         Vanilla L         |   0.650 (0.314)   |   0.682 (0.331)   |   0.756 (0.362)   |   0.891 (0.451)   |\n",
      "|         Vanilla NL        |   0.425 (0.260)   |   0.515 (0.296)   |   0.616 (0.351)   |   0.729 (0.433)   |\n",
      "|      Multibranch LANL     |   0.336 (0.182)   |   0.442 (0.254)   |   0.556 (0.323)   |   0.678 (0.408)   |\n",
      "|     Multibranch LANLD     |   0.288 (0.171)   |   0.390 (0.243)   |   0.499 (0.312)   |   0.623 (0.406)   |\n",
      "|      Transformer_DAE      |   0.273 (0.186)   |   0.351 (0.253)   |   0.422 (0.263)   |   0.506 (0.333)   |\n",
      "|    Transformer_COMBDAE    |   0.286 (0.194)   |   0.364 (0.266)   |   0.426 (0.273)   |   0.510 (0.344)   |\n",
      "| Transformer_COMBDAE_FreTS |   0.299 (0.197)   |   0.377 (0.261)   |   0.432 (0.263)   |   0.505 (0.328)   |\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "Printing Table for different noise values on the PRD metric\n",
      "\n",
      "\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|        Method/Model       | 0.2 < noise < 0.6 | 0.6 < noise < 1.0 | 1.0 < noise < 1.5 | 1.5 < noise < 2.0 |\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|         FIR Filter        |  53.247 (16.244)  |  73.910 (15.626)  |  84.868 (12.966)  |  90.785 (10.232)  |\n",
      "|         IIR Filter        |  51.858 (16.805)  |  72.437 (17.033)  |  83.801 (14.440)  |  89.756 (11.893)  |\n",
      "|            DRNN           |  53.075 (30.598)  |  59.322 (35.154)  |  66.180 (43.279)  |  74.467 (46.026)  |\n",
      "|          FCN-DAE          |  74.927 (55.071)  |  81.640 (57.841)  |  85.265 (66.679)  |  92.018 (67.156)  |\n",
      "|         Vanilla L         |  108.880 (43.270) |  110.031 (37.023) |  106.963 (28.889) |  105.430 (22.398) |\n",
      "|         Vanilla NL        |  74.730 (54.498)  |  92.785 (61.886)  |  103.172 (62.621) |  110.858 (61.043) |\n",
      "|      Multibranch LANL     |  57.444 (34.378)  |  75.869 (43.376)  |  89.270 (47.978)  |  99.731 (48.324)  |\n",
      "|     Multibranch LANLD     |  42.396 (19.796)  |  58.390 (30.363)  |  73.653 (41.313)  |  87.911 (45.718)  |\n",
      "|      Transformer_DAE      |  36.127 (17.521)  |  44.696 (22.674)  |  51.818 (26.858)  |  58.304 (29.762)  |\n",
      "|    Transformer_COMBDAE    |  37.518 (17.979)  |  45.908 (22.430)  |  52.340 (27.083)  |  58.057 (28.410)  |\n",
      "| Transformer_COMBDAE_FreTS |  38.606 (18.901)  |  47.275 (23.493)  |  52.339 (26.775)  |  58.031 (29.823)  |\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "Printing Table for different noise values on the COS SIM metric\n",
      "\n",
      "\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|        Method/Model       | 0.2 < noise < 0.6 | 0.6 < noise < 1.0 | 1.0 < noise < 1.5 | 1.5 < noise < 2.0 |\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|         FIR Filter        |   0.828 (0.109)   |   0.644 (0.164)   |   0.495 (0.186)   |   0.385 (0.189)   |\n",
      "|         IIR Filter        |   0.837 (0.107)   |   0.657 (0.167)   |   0.507 (0.191)   |   0.399 (0.197)   |\n",
      "|            DRNN           |   0.908 (0.081)   |   0.887 (0.101)   |   0.854 (0.140)   |   0.809 (0.180)   |\n",
      "|          FCN-DAE          |   0.866 (0.133)   |   0.816 (0.169)   |   0.764 (0.210)   |   0.700 (0.236)   |\n",
      "|         Vanilla L         |   0.744 (0.133)   |   0.650 (0.133)   |   0.554 (0.151)   |   0.466 (0.158)   |\n",
      "|         Vanilla NL        |   0.879 (0.086)   |   0.807 (0.126)   |   0.731 (0.173)   |   0.649 (0.209)   |\n",
      "|      Multibranch LANL     |   0.909 (0.063)   |   0.846 (0.100)   |   0.769 (0.153)   |   0.684 (0.194)   |\n",
      "|     Multibranch LANLD     |   0.933 (0.049)   |   0.884 (0.088)   |   0.817 (0.141)   |   0.737 (0.188)   |\n",
      "|      Transformer_DAE      |   0.944 (0.050)   |   0.914 (0.086)   |   0.882 (0.115)   |   0.840 (0.156)   |\n",
      "|    Transformer_COMBDAE    |   0.943 (0.049)   |   0.915 (0.079)   |   0.883 (0.109)   |   0.843 (0.146)   |\n",
      "| Transformer_COMBDAE_FreTS |   0.935 (0.060)   |   0.909 (0.079)   |   0.884 (0.110)   |   0.846 (0.151)   |\n",
      "+---------------------------+-------------------+-------------------+-------------------+-------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhc99/anaconda3/envs/DeepFilter/lib/python3.7/site-packages/ipykernel_launcher.py:93: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "rnd_test = np.load('rnd_test.npy')\n",
    "# rnd_test = np.concatenate([rnd_test, rnd_test])\n",
    "segm = [0.2, 0.6, 1.0, 1.5, 2.0]  # real number of segmentations is len(segmentations) - 1\n",
    "SSD_seg_all = []\n",
    "MAD_seg_all = []\n",
    "PRD_seg_all = []\n",
    "COS_SIM_seg_all = []\n",
    "for idx_exp in range(len(Exp_names)):\n",
    "    SSD_seg = [None] * (len(segm) - 1)\n",
    "    MAD_seg = [None] * (len(segm) - 1)\n",
    "    PRD_seg = [None] * (len(segm) - 1)\n",
    "    COS_SIM_seg = [None] * (len(segm) - 1)\n",
    "    for idx_seg in range(len(segm) - 1):\n",
    "        SSD_seg[idx_seg] = []\n",
    "        MAD_seg[idx_seg] = []\n",
    "        PRD_seg[idx_seg] = []\n",
    "        COS_SIM_seg[idx_seg] = []\n",
    "        for idx in range(len(rnd_test)):\n",
    "            # Object under analysis (oua)\n",
    "            # SSD\n",
    "            oua = SSD_all[idx_exp][idx]\n",
    "            if rnd_test[idx] > segm[idx_seg] and rnd_test[idx] < segm[idx_seg + 1]:\n",
    "                SSD_seg[idx_seg].append(oua)\n",
    "            # MAD\n",
    "            oua = MAD_all[idx_exp][idx]\n",
    "            if rnd_test[idx] > segm[idx_seg] and rnd_test[idx] < segm[idx_seg + 1]:\n",
    "                MAD_seg[idx_seg].append(oua)\n",
    "            # PRD\n",
    "            oua = PRD_all[idx_exp][idx]\n",
    "            if rnd_test[idx] > segm[idx_seg] and rnd_test[idx] < segm[idx_seg + 1]:\n",
    "                PRD_seg[idx_seg].append(oua)\n",
    "            # COS SIM\n",
    "            oua = CORR_all[idx_exp][idx]\n",
    "            if rnd_test[idx] > segm[idx_seg] and rnd_test[idx] < segm[idx_seg + 1]:\n",
    "                COS_SIM_seg[idx_seg].append(oua)\n",
    "    # Processing the last index\n",
    "    # SSD\n",
    "    SSD_seg[-1] = []\n",
    "    for idx in range(len(rnd_test)):\n",
    "        # Object under analysis\n",
    "        oua = SSD_all[idx_exp][idx]\n",
    "        if rnd_test[idx] > segm[-2]:\n",
    "            SSD_seg[-1].append(oua)\n",
    "    SSD_seg_all.append(SSD_seg)  # [exp][seg][item]\n",
    "    # MAD\n",
    "    MAD_seg[-1] = []\n",
    "    for idx in range(len(rnd_test)):\n",
    "        # Object under analysis\n",
    "        oua = MAD_all[idx_exp][idx]\n",
    "        if rnd_test[idx] > segm[-2]:\n",
    "            MAD_seg[-1].append(oua)\n",
    "    MAD_seg_all.append(MAD_seg)  # [exp][seg][item]\n",
    "    # PRD\n",
    "    PRD_seg[-1] = []\n",
    "    for idx in range(len(rnd_test)):\n",
    "        # Object under analysis\n",
    "        oua = PRD_all[idx_exp][idx]\n",
    "        if rnd_test[idx] > segm[-2]:\n",
    "            PRD_seg[-1].append(oua)\n",
    "    PRD_seg_all.append(PRD_seg)  # [exp][seg][item]\n",
    "    # COS SIM\n",
    "    COS_SIM_seg[-1] = []\n",
    "    for idx in range(len(rnd_test)):\n",
    "        # Object under analysis\n",
    "        oua = CORR_all[idx_exp][idx]\n",
    "        if rnd_test[idx] > segm[-2]:\n",
    "            COS_SIM_seg[-1].append(oua)\n",
    "    COS_SIM_seg_all.append(COS_SIM_seg)  # [exp][seg][item]\n",
    "# Printing Tables\n",
    "seg_table_column_name = []\n",
    "for idx_seg in range(len(segm) - 1):\n",
    "    column_name = str(segm[idx_seg]) + ' < noise < ' + str(segm[idx_seg + 1])\n",
    "    seg_table_column_name.append(column_name)\n",
    "# SSD Table\n",
    "SSD_seg_all = np.array(SSD_seg_all)\n",
    "SSD_seg_all = np.swapaxes(SSD_seg_all, 0, 1)\n",
    "print('\\n')\n",
    "print('Printing Table for different noise values on the SSD metric')\n",
    "generate_table(seg_table_column_name, SSD_seg_all, Exp_names)\n",
    "# MAD Table\n",
    "MAD_seg_all = np.array(MAD_seg_all)\n",
    "MAD_seg_all = np.swapaxes(MAD_seg_all, 0, 1)\n",
    "print('\\n')\n",
    "print('Printing Table for different noise values on the MAD metric')\n",
    "generate_table(seg_table_column_name, MAD_seg_all, Exp_names)\n",
    "# PRD Table\n",
    "PRD_seg_all = np.array(PRD_seg_all)\n",
    "PRD_seg_all = np.swapaxes(PRD_seg_all, 0, 1)\n",
    "print('\\n')\n",
    "print('Printing Table for different noise values on the PRD metric')\n",
    "generate_table(seg_table_column_name, PRD_seg_all, Exp_names)\n",
    "# COS SIM Table\n",
    "COS_SIM_seg_all = np.array(COS_SIM_seg_all)\n",
    "COS_SIM_seg_all = np.swapaxes(COS_SIM_seg_all, 0, 1)\n",
    "print('\\n')\n",
    "print('Printing Table for different noise values on the COS SIM metric')\n",
    "generate_table(seg_table_column_name, COS_SIM_seg_all, Exp_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepFilter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
